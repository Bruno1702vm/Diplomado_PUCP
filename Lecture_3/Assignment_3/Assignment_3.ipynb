{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Downloading pyreadstat-1.2.7-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\mauricio\\anaconda3\\lib\\site-packages (from pyreadstat) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mauricio\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mauricio\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mauricio\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mauricio\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mauricio\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n",
      "Downloading pyreadstat-1.2.7-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.4 MB 563.7 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.8/2.4 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.5/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.7/2.4 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.9/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.4 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "path = \"../../_data/endes/2019\"\n",
    "file1 = os.path.join(path, \"REC0111.sav\")\n",
    "file2 = os.path.join(path, \"RE223132.sav\")\n",
    "file3 = os.path.join(path, \"RE516171.sav\")\n",
    "\n",
    "rec_1 = pd.read_spss(\"../../_data/endes/2019/REC0111.sav\")\n",
    "rec_2 = pd.read_spss(\"../../_data/endes/2019/RE223132.sav\")\n",
    "rec_3 = pd.read_spss(\"../../_data/endes/2019/RE516171.sav\")\n",
    "\n",
    "\n",
    "#CREO QUE LO DE ABAJO ES EL MEJOR CÃ“DIGO\n",
    "\n",
    "# Import files with pyreadstat, capturing both the dataframe and metadata\n",
    "rec_1, meta1 = pyreadstat.read_sav(\"../../_data/endes/2019/REC0111.sav\")\n",
    "rec_2, meta2 = pyreadstat.read_sav(\"../../_data/endes/2019/RE223132.sav\")\n",
    "rec_3, meta3 = pyreadstat.read_sav(\"../../_data/endes/2019/RE516171.sav\")\n",
    "\n",
    "# Extract variable and value labels\n",
    "var_labels1 = meta1.column_labels\n",
    "value_labels1 = meta1.value_labels\n",
    "\n",
    "var_labels2 = meta2.column_labels\n",
    "value_labels2 = meta2.value_labels\n",
    "\n",
    "var_labels3 = meta3.column_labels\n",
    "value_labels3 = meta3.value_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files with pyreadstat, capturing both the dataframe and metadata\n",
    "rec_1, meta1 = pyreadstat.read_sav(\"../../_data/endes/2019/REC0111.sav\")\n",
    "rec_2, meta2 = pyreadstat.read_sav(\"../../_data/endes/2019/RE223132.sav\")\n",
    "rec_3, meta3 = pyreadstat.read_sav(\"../../_data/endes/2019/RE516171.sav\")\n",
    "\n",
    "# Extract variable and value labels\n",
    "var_labels1 = meta1.column_labels\n",
    "value_labels1 = meta1.value_labels\n",
    "\n",
    "var_labels2 = meta2.column_labels\n",
    "value_labels2 = meta2.value_labels\n",
    "\n",
    "var_labels3 = meta3.column_labels\n",
    "value_labels3 = meta3.value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set. Check if all the columns are in the dataset. Make a code that check the columns that are not included. Please, reporte them.\n",
    "\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter, `for loop`,   and [this link](https://stackoverflow.com/questions/3420122/filter-dict-to-contain-only-certain-keys) to update the var and value dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to select for each dataset\n",
    "cols_rec1 = [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\", \"V124\", \"V125\", \"V127\", \"V133\"]\n",
    "cols_rec2 = [\"CASEID\", \"V201\", \"V218\", \"V301\", \"V302\", \"V323\", \"V323A\", \"V325A\", \"V326\", \"V327\", \"V337\", \"V359\", \"V360\", \"V361\", \"V362\", \"V363\", \"V364\", \"V367\", \"V372\", \"V372A\", \"V375A\", \"V376\", \"V376A\", \"V379\", \"V380\"]\n",
    "cols_rec3 = [\"CASEID\", \"V501\", \"V502\", \"V503\", \"V504\", \"V505\", \"V506\", \"V507\", \"V508\", \"V509\", \"V510\", \"V511\", \"V512\", \"V513\", \"V525\", \"V613\", \"V714\", \"V715\"]\n",
    "\n",
    "# Function to select and check for missing columns\n",
    "def check_and_select_cols(df, cols, dataset_name):\n",
    "    # Select columns that are present in the dataframe\n",
    "    selected_cols = [col for col in cols if col in df.columns]\n",
    "    # Identify columns that are missing from the dataframe\n",
    "    missing_cols = [col for col in cols if col not in df.columns]\n",
    "    \n",
    "    # If there are missing columns, print a message\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns for {dataset_name}: {missing_cols}\")\n",
    "    \n",
    "    # Return the dataframe with only the selected columns\n",
    "    return df.loc[:, selected_cols]\n",
    "\n",
    "# Applying function to each dataset\n",
    "rec1_1 = check_and_select_cols(rec_1, cols_rec1, \"rec1\")\n",
    "rec2_1 = check_and_select_cols(rec_2, cols_rec2, \"rec2\")\n",
    "rec3_1 = check_and_select_cols(rec_3, cols_rec3, \"rec3\")\n",
    "\n",
    "# Function to update variable labels for selected columns\n",
    "def update_labels(old_labels, selected_cols):\n",
    "    # Create a new dictionary with only the selected columns\n",
    "    updated_labels = {key: old_labels[key] for key in selected_cols if key in old_labels}\n",
    "    return updated_labels\n",
    "\n",
    "# Updating variable labels\n",
    "new_var_labels1 = update_labels(var_labels1, cols_rec1)\n",
    "new_var_labels2 = update_labels(var_labels2, cols_rec2)\n",
    "new_var_labels3 = update_labels(var_labels3, cols_rec3)\n",
    "\n",
    "# Function to update value labels for selected columns\n",
    "def update_value_labels(old_labels, selected_cols):\n",
    "    # Create a new dictionary to hold updated value labels\n",
    "    updated_labels = {key: old_labels[key] for key in selected_cols if key in old_labels}\n",
    "    return updated_labels\n",
    "\n",
    "# Updating value labels\n",
    "new_value_labels1 = update_value_labels(value_labels1, cols_rec1)\n",
    "new_value_labels2 = update_value_labels(value_labels2, cols_rec2)\n",
    "new_value_labels3 = update_value_labels(value_labels3, cols_rec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column 'year' to rec1_1 with the value 2019\n",
    "rec1_1.loc[:, 'year'] = 2019\n",
    "\n",
    "# Update the variable labels to include the new 'year' column\n",
    "new_var_labels1.update({'year': \"Year of the survey\"})\n",
    "\n",
    "print(new_var_labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge the datasets using the 'CASEID' column as our common variable (on = 'CASEID') and indicating 'outer' as the method\n",
    " # By using 'outer' we make sure that all our selected variables are preserved in the new dataset\n",
    " # Because 'outer' employs the union (all variables), whereas 'inner' only considers the intersection (shared variables)\n",
    "endes_2019 = pd.merge(rec1_1, rec2_1, on = 'CASEID', how = 'outer')\n",
    "endes_2019 = pd.merge(endes_2019, rec3_1, on = 'CASEID', how = 'outer')\n",
    "\n",
    "# Let's check if everything is in order\n",
    "endes_2019.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To combine all new_var_labels into a single dictionary\n",
    "var_labels = new_var_labels1.copy()\n",
    "var_labels.update(new_var_labels2)\n",
    "var_labels.update(new_var_labels3)\n",
    "\n",
    "# Now, we do the same for all new_value_labels\n",
    "value_labels = new_value_labels1.copy()\n",
    "value_labels.update(new_value_labels2)\n",
    "value_labels.update(new_value_labels3)\n",
    "\n",
    "# Finally, we add the new attributes to the endes_2019 DataFrame\n",
    "endes_2019.attrs['var_labels'] = var_labels\n",
    "endes_2019.attrs['value_labels'] = value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, replicate your code of the prevoius sections but for years **2019, 2018, 2017, 2016, 2015**. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their **variables and values labels** from this path `\"../../_data/endes/\"`. For this excersie you must use a for loop. This loop must iterate over **2019, 2018, 2017, 2016, 2015 folders** and import these files. All the files have the same name. You must store these files and their labels in a nested dictionary named as `all_data`. The keys of the dictionary should be named as `year_2019`, for example, and the keys of the nested dictionary should be `data`, `var_labels`, and `value_labels`. **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the years we are interested in and the base path for the data\n",
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "base_path = \"../../_data/endes/\"\n",
    "\n",
    "# Initialize an empty dictionary to store all data\n",
    "all_data = {}\n",
    "\n",
    "# OK, now we loop through each year to import data and labels\n",
    "for year in years:\n",
    "    # First, we create a string for the dictionary key that includes the year\n",
    "    year_str = f\"year_{year}\"\n",
    "    \n",
    "    # Then, let's build the path to the directory for the current year\n",
    "    path = f\"{base_path}{year}/\"\n",
    "    \n",
    "    # Now, we define the file paths for each dataset within the current year's directory\n",
    "    file1 = f\"{path}REC0111.sav\"\n",
    "    file2 = f\"{path}RE223132.sav\"\n",
    "    file3 = f\"{path}RE516171.sav\"\n",
    "    \n",
    "    # Next, we import the data and labels for each file using pyreadstat\n",
    "    rec1, meta1 = pyreadstat.read_sav(file1)\n",
    "    rec2, meta2 = pyreadstat.read_sav(file2)\n",
    "    rec3, meta3 = pyreadstat.read_sav(file3)\n",
    "    \n",
    "    # Finally, we store the imported data and labels in a nested dictionary\n",
    "    # We will use 'data', 'var_labels', and 'value_labels' as keys for the nested dictionary\n",
    "    all_data[year_str] = {\n",
    "        # 'data' will store the actual dataframes\n",
    "        'data': [rec1, rec2, rec3],\n",
    "        # 'var_labels' will store the variable labels for each dataframe\n",
    "        'var_labels': [meta1.column_labels, meta2.column_labels, meta3.column_labels],\n",
    "        # 'value_labels' will store the value labels for each dataframe\n",
    "        'value_labels': [meta1.value_labels, meta2.value_labels, meta3.value_labels]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `all_data` to append all the data sets. Store all data sets in a list using `for loop`. Then, use `pd.concat` to append all the data sets. Also, you must reset the index to have a good-looking data. This new object should be named as `endes_data_2015_2019`. **Hint: Use [this code](https://stackoverflow.com/questions/32444138/concatenate-a-list-of-pandas-dataframes-together)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns for year 2018: \"['V007'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store dataframes\n",
    "data_frames = []\n",
    "\n",
    "# Iterate through the years and collect the dataframes\n",
    "for year in years:\n",
    "    year_str = f\"year_{year}\"\n",
    "    rec1, rec2, rec3 = all_data[year_str]['data']\n",
    "    \n",
    "    # Select relevant columns for each dataframe and use .loc to avoid SettingWithCopyWarning\n",
    "    try:\n",
    "        rec1_1 = rec1.loc[:, cols_rec1]\n",
    "        rec2_1 = rec2.loc[:, cols_rec2]\n",
    "        rec3_1 = rec3.loc[:, cols_rec3]\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing columns for year {year}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Add a new column 'year' to each dataframe\n",
    "    rec1_1.loc[:, 'year'] = year\n",
    "    rec2_1.loc[:, 'year'] = year\n",
    "    rec3_1.loc[:, 'year'] = year\n",
    "    \n",
    "    # Merge the three datasets on 'CASEID'\n",
    "    merged_df = pd.merge(rec1_1, rec2_1, on='CASEID', how='inner')\n",
    "    merged_df = pd.merge(merged_df, rec3_1, on='CASEID', how='inner')\n",
    "    \n",
    "    # Append the merged dataframe to the list\n",
    "    data_frames.append(merged_df)\n",
    "\n",
    "# Concatenate all dataframes in the list\n",
    "endes_data_2015_2019 = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Reset the index to have a clean dataframe\n",
    "endes_data_2015_2019.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns for rec1 in year 2018: {'V007'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store dataframes\n",
    "data_frames = []\n",
    "\n",
    "# Iterate through the years and collect the dataframes\n",
    "for year in years:\n",
    "    year_str = f\"year_{year}\"\n",
    "    rec1, rec2, rec3 = all_data[year_str]['data']\n",
    "    \n",
    "    # Check and select only the columns that exist in the dataframe\n",
    "    rec1_cols = [col for col in cols_rec1 if col in rec1.columns]\n",
    "    rec2_cols = [col for col in cols_rec2 if col in rec2.columns]\n",
    "    rec3_cols = [col for col in cols_rec3 if col in rec3.columns]\n",
    "    \n",
    "    if len(rec1_cols) < len(cols_rec1):\n",
    "        missing_cols = set(cols_rec1) - set(rec1_cols)\n",
    "        print(f\"Missing columns for rec1 in year {year}: {missing_cols}\")\n",
    "    if len(rec2_cols) < len(cols_rec2):\n",
    "        missing_cols = set(cols_rec2) - set(rec2_cols)\n",
    "        print(f\"Missing columns for rec2 in year {year}: {missing_cols}\")\n",
    "    if len(rec3_cols) < len(cols_rec3):\n",
    "        missing_cols = set(cols_rec3) - set(rec3_cols)\n",
    "        print(f\"Missing columns for rec3 in year {year}: {missing_cols}\")\n",
    "    \n",
    "    rec1_1 = rec1.loc[:, rec1_cols]\n",
    "    rec2_1 = rec2.loc[:, rec2_cols]\n",
    "    rec3_1 = rec3.loc[:, rec3_cols]\n",
    "    \n",
    "    # Add a new column 'year' to each dataframe\n",
    "    rec1_1.loc[:, 'year'] = year\n",
    "    rec2_1.loc[:, 'year'] = year\n",
    "    rec3_1.loc[:, 'year'] = year\n",
    "    \n",
    "    # Merge the three datasets on 'CASEID'\n",
    "    merged_df = pd.merge(rec1_1, rec2_1, on='CASEID', how='inner')\n",
    "    merged_df = pd.merge(merged_df, rec3_1, on='CASEID', how='inner')\n",
    "    \n",
    "    # Append the merged dataframe to the list\n",
    "    data_frames.append(merged_df)\n",
    "\n",
    "# Concatenate all dataframes in the list\n",
    "endes_data_2015_2019 = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Reset the index to have a clean dataframe\n",
    "endes_data_2015_2019.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Store all the `var_labels` and `value_labels` in a dictionary named as `all_var_labels` and `all_value_labels`. The first keys should be the year for both dictionaries.Then, use them to generate new attributes for `endes_data_2015_2019`. These attributes should be named as `var_labels` and `value_labels`.  **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.3.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Use `endes_data_2015_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `mean_key_vars` with `endes_data_2015_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
