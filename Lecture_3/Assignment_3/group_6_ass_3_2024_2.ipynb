{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadstat in c:\\users\\micae\\miniconda3\\lib\\site-packages (1.2.7)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\micae\\miniconda3\\lib\\site-packages (from pyreadstat) (1.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\micae\\miniconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\micae\\miniconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\micae\\miniconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\micae\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadstat) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import each file using \"pyreadstat\" to read SPSS files and get 2 objects, a dataframe and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pyreadstat._readstat_parser.metadata_container'>\n"
     ]
    }
   ],
   "source": [
    "# First, we imported \"REC0111.sav\", \"RE223132.sav\", \"RE516171.sav\" files and they're turned into  into dataframes and metadata.\n",
    "rec_1,meta1 = pyreadstat.read_sav('../../_data/endes/2019/REC0111.sav')\n",
    "rec_2,meta2 = pyreadstat.read_sav('../../_data/endes/2019/RE223132.sav')\n",
    "rec_3,meta3 = pyreadstat.read_sav('../../_data/endes/2019/RE516171.sav')\n",
    "\n",
    "print(type(rec_1))\n",
    "print(type(meta1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, variable and value labels are being extracted from the metadata objects associated with each dataset. 'var_labels1', 'var_labels2', and 'var_labels3' store the column (variable) labels, while 'value_labels1', 'value_labels2', and 'value_labels3' store the value labels for each respective dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we get labels from metadata (meta1, meta2, meta3)\n",
    "# For rec1:\n",
    "var_labels1   = meta1.column_names_to_labels      # extract variable labels\n",
    "value_labels1 = meta1.variable_value_labels     # extract variables' value labels \n",
    "# For rec2:\n",
    "var_labels2   = meta2.column_names_to_labels\n",
    "value_labels2 = meta2.variable_value_labels\n",
    "# For rec3:\n",
    "var_labels3   = meta3.column_names_to_labels\n",
    "value_labels3 = meta3.variable_value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set. Check if all the columns are in the dataset. Make a code that check the columns that are not included. Please, reporte them.\n",
    "\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter, `for loop`,   and [this link](https://stackoverflow.com/questions/3420122/filter-dict-to-contain-only-certain-keys) to update the var and value dictionary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates new df objects (rec1_1, rec2_1, rec3_1) by selecting specific columns from the original datasets `rec_1`, `rec_2`, and `rec_3`. Each df includes only the columns listed, focusing on specific variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now,  we use \"loc\" to keep all rows but only include the columns listed in the specified column lists. \n",
    "# loc[:, <column_list>] syntax selects all rows and filters the df to retain only specific columns.\n",
    "rec1_1 = rec_1.loc[:,[\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \n",
    "                      \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\", \"V124\", \"V125\", \"V127\", \"V133\"]]\n",
    "rec2_1 = rec_2.loc[:,[\"CASEID\", \"V201\", \"V218\", \"V301\", \"V302\", \"V323\", \"V323A\", \"V325A\", \"V326\", \"V327\", \"V337\", \n",
    "                      \"V359\", \"V360\", \"V361\", \"V362\", \"V363\", \"V364\", \"V367\", \"V372\", \"V372A\", \"V375A\", \"V376\", \n",
    "                      \"V376A\", \"V379\", \"V380\"]]\n",
    "rec3_1 = rec_3.loc[:,[\"CASEID\", \"V501\", \"V502\", \"V503\", \"V504\", \"V505\", \"V506\", \"V507\", \"V508\", \"V509\", \"V510\",\n",
    "                      \"V511\", \"V512\", \"V513\", \"V525\", \"V613\", \"V714\", \"V715\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASEID</th>\n",
       "      <th>V000</th>\n",
       "      <th>V001</th>\n",
       "      <th>V002</th>\n",
       "      <th>V003</th>\n",
       "      <th>V004</th>\n",
       "      <th>V007</th>\n",
       "      <th>V008</th>\n",
       "      <th>V009</th>\n",
       "      <th>V010</th>\n",
       "      <th>...</th>\n",
       "      <th>V024</th>\n",
       "      <th>V102</th>\n",
       "      <th>V120</th>\n",
       "      <th>V121</th>\n",
       "      <th>V122</th>\n",
       "      <th>V123</th>\n",
       "      <th>V124</th>\n",
       "      <th>V125</th>\n",
       "      <th>V127</th>\n",
       "      <th>V133</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000100201  3</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000102801  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000102801  6</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000104801  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               CASEID V000  V001  V002  V003  V004    V007    V008  V009  \\\n",
       "0        000100201  2  PE6   1.0   2.0   2.0   1.0  2019.0  1434.0   4.0   \n",
       "1        000100201  3  PE6   1.0   2.0   3.0   1.0  2019.0  1434.0   1.0   \n",
       "2        000102801  2  PE6   1.0  28.0   2.0   1.0  2019.0  1434.0   6.0   \n",
       "3        000102801  6  PE6   1.0  28.0   6.0   1.0  2019.0  1434.0   3.0   \n",
       "4        000104801  2  PE6   1.0  48.0   2.0   1.0  2019.0  1434.0   5.0   \n",
       "\n",
       "     V010  ...  V024  V102  V120  V121  V122  V123  V124  V125  V127  V133  \n",
       "0  1986.0  ...   1.0   1.0   1.0   1.0   1.0   0.0   0.0   0.0  33.0  16.0  \n",
       "1  2007.0  ...   1.0   1.0   1.0   1.0   1.0   0.0   0.0   0.0  33.0   6.0  \n",
       "2  1983.0  ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  33.0  16.0  \n",
       "3  1970.0  ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  33.0   4.0  \n",
       "4  1991.0  ...   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  34.0   1.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we use head to check the result\n",
    "rec1_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rec1_1: True\n",
      "For rec2_1: True\n",
      "For rec3_1: True\n"
     ]
    }
   ],
   "source": [
    "# Then we check if all the columns are in the dataset:\n",
    "print(\"For rec1_1:\", list(rec1_1.columns) == [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \n",
    "                      \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\", \"V124\", \"V125\", \"V127\", \"V133\"])\n",
    "print(\"For rec2_1:\", list(rec2_1.columns) == [\"CASEID\", \"V201\", \"V218\", \"V301\", \"V302\", \"V323\", \"V323A\", \"V325A\", \"V326\", \"V327\", \"V337\", \n",
    "                      \"V359\", \"V360\", \"V361\", \"V362\", \"V363\", \"V364\", \"V367\", \"V372\", \"V372A\", \"V375A\", \"V376\", \n",
    "                      \"V376A\", \"V379\", \"V380\"])\n",
    "print(\"For rec3_1:\", list(rec3_1.columns) == [\"CASEID\", \"V501\", \"V502\", \"V503\", \"V504\", \"V505\", \"V506\", \"V507\", \"V508\", \"V509\", \"V510\",\n",
    "                      \"V511\", \"V512\", \"V513\", \"V525\", \"V613\", \"V714\", \"V715\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code identifies columns in the rec_1 df that are not included in the rec1_1 df by comparing against a predefined list of selected columns. It creates a list of excluded columns and prints it. It also prints the number of selected columns and the number of excluded columns to verify the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID1', 'HHID', 'V013', 'V014', 'V015', 'V017', 'V018', 'V019', 'V019A', 'V020', 'V021', 'V023', 'V025', 'V026', 'V027', 'V028', 'V029', 'V030', 'V031', 'V032', 'V033', 'V034', 'V040', 'V042', 'V043', 'V044', 'Q105DD', 'V101', 'V103', 'V104', 'V105', 'V106', 'V107', 'V113', 'V115', 'V116', 'V119', 'V128', 'V129', 'V130', 'V131', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V149', 'V150', 'V151', 'V152', 'V153', 'AWFACTT', 'AWFACTU', 'AWFACTR', 'AWFACTE', 'AWFACTW', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V166', 'V167', 'V168', 'ML101', 'QD333_1', 'QD333_2', 'QD333_3', 'QD333_4', 'QD333_5', 'QD333_6', 'UBIGEO', 'V022', 'V005', 'V190', 'V191', 'mujeres12a49', 'NCONGLOME']\n",
      "N° of variables of selected columns 22\n",
      "N° of variables of excluded columns 83\n"
     ]
    }
   ],
   "source": [
    "# Now, we can check the columns that are not included in \"rec1_1\":\n",
    "# Also, we define the list of columns that were selected to be included in rec1_1\n",
    "selected = [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \n",
    "                      \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\", \"V124\", \"V125\", \"V127\", \"V133\"]\n",
    "\n",
    "# Then we create a list of columns in rec_1 that are not in the 'selected' list\n",
    "# The list comprehension iterates over all column names in rec_1.columns and includes those not found in the 'selected' list\n",
    "\n",
    "columns_not_included = [col for col in rec_1.columns if col not in selected]    # list of columns that are not included\n",
    "\n",
    "print(columns_not_included)\n",
    "# We checked it using len()\n",
    "print(\"N° of variables of selected columns\",len(selected))\n",
    "print(\"N° of variables of excluded columns\", len(columns_not_included))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('ID1', 'Año'), ('HHID', 'Identificación Cuestionario del Hogar'), ('CASEID', 'Identificación Cuestionario Individual'), ('V001', 'Conglomerado'), ('V002', 'Número de vivienda'), ('V003', 'Número de línea de entrevistada'), ('V004', 'Unidad de área final'), ('V007', 'Año de la entrevista'), ('V008', 'Fecha de la entrevista, Codificación centenaria de meses (CMC)'), ('V009', 'Mes de nacimiento de la entrevistada'), ('V010', 'Año de nacimiento de la entrevistada'), ('V011', 'Fecha de nacimiento, Codificación centenaria de meses (CMC)'), ('V012', 'Edad actual - entrevistada'), ('V013', 'Edad actual por grupos de 5 años'), ('V014', 'Integridad de la información para la fecha de nacimiento'), ('V015', 'Resultado entrevista individual'), ('V017', 'Inicio del calendario, Codificación centenaria de mesesl CMC'), ('V018', 'Columna del mes de la entrevista'), ('V019', 'Duración del calendario'), ('V019A', 'Número de columnas de calendario'), ('V020', 'Muestra alguna vez casada'), ('V021', 'Unidad de muestreo primario - conglomerado'), ('V023', 'Dominio de ejemplo - Departamento'), ('V024', 'Región'), ('V025', 'Tipo de lugar de residencia'), ('V026', 'El lugar de residencia en el que se entrevistó - De Facto'), ('V027', 'Número de visitas'), ('V028', 'Identificación del entrevistador'), ('V029', 'Identificador del digitador'), ('V030', 'Supervisor de campo'), ('V031', 'Editor de campo'), ('V032', 'Editor de la oficina'), ('V033', 'Selección final del área de probabilidad'), ('V034', 'Número de orden del esposo'), ('V040', 'Altitud del conglomerado en metros'), ('V042', 'Selección de hogar para hemoglobina'), ('V043', 'Selección para módulo de estatus de mujeres'), ('V044', 'Selección para módulo de violencia domestica'), ('V000', 'Código y fase del país'), ('Q105DD', 'Dia de nacimeinto de la entrevistada'), ('V101', 'Región'), ('V102', 'Tipo de lugar de residencia'), ('V103', 'Lugar de residencia de la infancia'), ('V104', 'Cuanto tiempo tiene viviendo continuamente en el lugar de residencia actual'), ('V105', 'Tipo de lugar de residencia anteriormente'), ('V106', 'Nivel educativo más alto'), ('V107', 'Año/grado de educación más alto aprobado'), ('V113', 'Fuente principal de abasteciemiento de agua potable que utilizan en su hogar para tomar o beber'), ('V115', 'Tiempo para llegar a la fuente de agua'), ('V116', 'Tipo de instalación sanitaria'), ('V119', 'En su hogar tiene: electricidad'), ('V120', 'En su hogar tiene: radio'), ('V121', 'En su hogar tiene: televisión'), ('V122', 'En su hogar tiene: refrigerador'), ('V123', 'En su hogar tiene: bicicleta'), ('V124', 'En su hogar tiene: motocicleta/motocar'), ('V125', 'En su hogar tiene: coche/camión'), ('V127', 'Material predominante del piso de la vivienda'), ('V128', 'Material predominante de las paredes exteriores de la vivienda'), ('V129', 'Material predominante del techo de la vivienda'), ('V130', 'Religión'), ('V131', 'Etnicidad'), ('V133', 'Educación en años simples'), ('V134', 'El lugar en el que se realizó la entrevista  De-facto'), ('V135', 'Residente habitual o visitante'), ('V136', 'Número de miembros del hogar'), ('V137', 'Número de niños de 6 años de edad'), ('V138', 'Número de mujeres de 15 a 49 años de edad elegibles en el hogar'), ('V139', 'Región, residencia habitual De-jure'), ('V140', 'Tipo de área de residencia De-jure'), ('V141', 'Lugar de residencia De-jure'), ('V149', 'Logro educativo'), ('V150', 'Relación con el jefe del hogar'), ('V151', 'Sexo del Jefe del Hogar'), ('V152', 'Edad del jefe del hogar'), ('V153', 'En su hogar tiene: teléfono'), ('AWFACTT', 'Factor todas las mujeres - total'), ('AWFACTU', 'Factor todas las mujeres - urbano/rural'), ('AWFACTR', 'Factor todas las mujeres - regional'), ('AWFACTE', 'Factor todas las mujeres - educación'), ('AWFACTW', 'Factor todas las mujeres - índice de riqueza'), ('V155', 'Alfabetización'), ('V156', 'Alguna vez participó en un programa de alfabetización (no incluyendo la escuela primaria)'), ('V157', 'Frecuencia de lectura de un periódico o revista'), ('V158', 'Frecuencia de escuchar radio'), ('V159', 'Frecuencia de ver televisión'), ('V160', 'Baño compartido con otros hogares'), ('V161', 'Tipo de combustible para cocinar'), ('V166', 'Resultados de la prueba del yodo en la sal'), ('V167', 'Número de viajes en los últimos 12 meses'), ('V168', 'Afuera más de un mes en los últimos 12 meses'), ('ML101', 'Tipo de mosquitero que utilizo para dormir última noche'), ('QD333_1', 'Alguna dificultad o limitación permanente para ver, aún usando anteojos'), ('QD333_2', 'Alguna dificultad o limitación permanente para oir, aún usando audífonos'), ('QD333_3', 'Alguna dificultad o limitación permanente para hablar o comunicarse, aún usando la lengua de señas u otro'), ('QD333_4', 'Alguna dificultad o limitación permanente para moverse o caminar para usar brazos y/o piernas'), ('QD333_5', 'Alguna dificultad o limitación permanente para entender o aprender (concentrarse y recordarse)'), ('QD333_6', 'Alguna dificultad o limitación permanente para relacionarse con los demás, por sus pensamientos, sentimientos, emociones o conductas'), ('UBIGEO', 'Código de Ubicación Gegráfica'), ('V022', 'Estratos'), ('V005', 'Factor de ponderacion'), ('V190', 'Índice de riqueza'), ('V191', 'Factor de puntuación del índice de riqueza (5 decimales)'), ('mujeres12a49', 'Mujeres de 12 a 49 años de edad'), ('NCONGLOME', 'Número de Conglomerado (proveniente del marco)')])\n"
     ]
    }
   ],
   "source": [
    "# Now we update variable and value labels by filtering to keep only those labels that match the selected columns.\n",
    "# The code creates new dictionaries (`new_var_labels1`, `new_var_labels2`, `new_var_labels3`) by including only key-value pairs from `var_labels1`, `var_labels2`, and `var_labels3` where the key (representing a variable) is present in the columns of `rec1_1`, `rec2_1`, and `rec3_1`, respectively. \n",
    "# items() returns a view of the dictionary's key-value pairs, which is used to filter the labels based on column presence.\n",
    "\n",
    "print(var_labels1.items())\n",
    "#\n",
    "new_var_labels1 = {key: value for key, value in var_labels1.items() if key in list(rec1_1.columns)}    \n",
    "new_var_labels2 = {key: value for key, value in var_labels2.items() if key in list(rec2_1.columns)}\n",
    "new_var_labels3 = {key: value for key, value in var_labels3.items() if key in list(rec3_1.columns)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters the value_labels1, value_labels2, and value_labels3 dictionaries to include only those key-value pairs where the key (representing a variable) exists in the columns of rec1_1, rec2_1, and rec3_1, respectively. It creates new dictionaries (new_value_labels1, new_value_labels2, new_value_labels3) that retain only the relevant value labels based on the columns present in each df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value_labels1 = {key: value for key, value in value_labels1.items() if key in list(rec1_1.columns)}\n",
    "new_value_labels2 = {key: value for key, value in value_labels2.items() if key in list(rec2_1.columns)}\n",
    "new_value_labels3 = {key: value for key, value in value_labels3.items() if key in list(rec3_1.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CASEID': 'Identificación Cuestionario Individual',\n",
       " 'V001': 'Conglomerado',\n",
       " 'V002': 'Número de vivienda',\n",
       " 'V003': 'Número de línea de entrevistada',\n",
       " 'V004': 'Unidad de área final',\n",
       " 'V007': 'Año de la entrevista',\n",
       " 'V008': 'Fecha de la entrevista, Codificación centenaria de meses (CMC)',\n",
       " 'V009': 'Mes de nacimiento de la entrevistada',\n",
       " 'V010': 'Año de nacimiento de la entrevistada',\n",
       " 'V011': 'Fecha de nacimiento, Codificación centenaria de meses (CMC)',\n",
       " 'V012': 'Edad actual - entrevistada',\n",
       " 'V024': 'Región',\n",
       " 'V000': 'Código y fase del país',\n",
       " 'V102': 'Tipo de lugar de residencia',\n",
       " 'V120': 'En su hogar tiene: radio',\n",
       " 'V121': 'En su hogar tiene: televisión',\n",
       " 'V122': 'En su hogar tiene: refrigerador',\n",
       " 'V123': 'En su hogar tiene: bicicleta',\n",
       " 'V124': 'En su hogar tiene: motocicleta/motocar',\n",
       " 'V125': 'En su hogar tiene: coche/camión',\n",
       " 'V127': 'Material predominante del piso de la vivienda',\n",
       " 'V133': 'Educación en años simples'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_var_labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V024': {1.0: 'Amazonas',\n",
       "  2.0: 'Ancash',\n",
       "  3.0: 'Apurimac',\n",
       "  4.0: 'Arequipa',\n",
       "  5.0: 'Ayacucho',\n",
       "  6.0: 'Cajamarca',\n",
       "  7.0: 'Callao',\n",
       "  8.0: 'Cusco',\n",
       "  9.0: 'Huancavelica',\n",
       "  10.0: ' Huanuco',\n",
       "  11.0: ' Ica',\n",
       "  12.0: ' Junin',\n",
       "  13.0: ' La Libertad',\n",
       "  14.0: ' Lambayeque',\n",
       "  15.0: ' Lima',\n",
       "  16.0: ' Loreto',\n",
       "  17.0: ' Madre de Dios',\n",
       "  18.0: ' Moquegua',\n",
       "  19.0: ' Pasco',\n",
       "  20.0: ' Piura',\n",
       "  21.0: ' Puno',\n",
       "  22.0: ' San Martin',\n",
       "  23.0: ' Tacna',\n",
       "  24.0: ' Tumbes',\n",
       "  25.0: ' Ucayali'},\n",
       " 'V102': {1.0: 'Urbano', 2.0: 'Rural'},\n",
       " 'V120': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V121': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V122': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V123': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V124': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V125': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V127': {11.0: 'Tierra/arena',\n",
       "  21.0: 'Madera (entablados)',\n",
       "  31.0: 'Parquet o madera pulida',\n",
       "  32.0: 'Làminas asfálticas, vinílicos o similares',\n",
       "  33.0: 'Losetas, terrazos o similares',\n",
       "  34.0: 'Cemento/ladrillo',\n",
       "  96.0: 'Pona',\n",
       "  97.0: 'Otro(pona)'},\n",
       " 'V133': {97.0: 'Inconsistente'}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_value_labels1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we updated the variable and value dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'year' to rec1_1 using loc, setting all values to 2019\n",
    "rec1_1.loc[:, 'year'] = 2019\n",
    "\n",
    "# Update the new_var_labels1 dictionary with the new 'year' column using the update method\n",
    "new_var_labels1.update({'year': \"Year of the survey\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CASEID': 'Identificación Cuestionario Individual',\n",
       " 'V001': 'Conglomerado',\n",
       " 'V002': 'Número de vivienda',\n",
       " 'V003': 'Número de línea de entrevistada',\n",
       " 'V004': 'Unidad de área final',\n",
       " 'V007': 'Año de la entrevista',\n",
       " 'V008': 'Fecha de la entrevista, Codificación centenaria de meses (CMC)',\n",
       " 'V009': 'Mes de nacimiento de la entrevistada',\n",
       " 'V010': 'Año de nacimiento de la entrevistada',\n",
       " 'V011': 'Fecha de nacimiento, Codificación centenaria de meses (CMC)',\n",
       " 'V012': 'Edad actual - entrevistada',\n",
       " 'V024': 'Región',\n",
       " 'V000': 'Código y fase del país',\n",
       " 'V102': 'Tipo de lugar de residencia',\n",
       " 'V120': 'En su hogar tiene: radio',\n",
       " 'V121': 'En su hogar tiene: televisión',\n",
       " 'V122': 'En su hogar tiene: refrigerador',\n",
       " 'V123': 'En su hogar tiene: bicicleta',\n",
       " 'V124': 'En su hogar tiene: motocicleta/motocar',\n",
       " 'V125': 'En su hogar tiene: coche/camión',\n",
       " 'V127': 'Material predominante del piso de la vivienda',\n",
       " 'V133': 'Educación en años simples',\n",
       " 'year': 'Year of the survey'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_var_labels1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the loc method to add a 'year' column ensures precise data handling within the rec1_1 DataFrame, making sure that the new column is correctly populated with the value 2019 for all rows. Additionally, updating the new_var_labels1 dictionary using the update method maintains the integrity of metadata, which is essential for clear documentation and future data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unión externa entre rec1_1 y rec2_1\n",
    "endes_2019 = rec1_1.merge(rec2_1, on=\"CASEID\", how='outer')\n",
    "\n",
    "# Unión externa entre el resultado anterior y rec3_1\n",
    "endes_2019 = endes_2019.merge(rec3_1, on=\"CASEID\", how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0            000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
      "1            000100201  3  PE6     1.0   2.0   3.0     1.0  2019.0  1434.0   \n",
      "2            000102801  2  PE6     1.0  28.0   2.0     1.0  2019.0  1434.0   \n",
      "3            000102801  6  PE6     1.0  28.0   6.0     1.0  2019.0  1434.0   \n",
      "4            000104801  2  PE6     1.0  48.0   2.0     1.0  2019.0  1434.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "38330        325406201  2  PE6  3254.0  62.0   2.0  3254.0  2019.0  1440.0   \n",
      "38331        325406301  2  PE6  3254.0  63.0   2.0  3254.0  2019.0  1440.0   \n",
      "38332        325407001  2  PE6  3254.0  70.0   2.0  3254.0  2019.0  1440.0   \n",
      "38333        325407201  2  PE6  3254.0  72.0   2.0  3254.0  2019.0  1440.0   \n",
      "38334        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0       4.0  1986.0  ...  2008.0  1297.0   1.0  21.0  11.0   3.0  17.0   2.0   \n",
      "1       1.0  2007.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "2       6.0  1983.0  ...  2011.0  1344.0   1.0  28.0   7.0   2.0  18.0   3.0   \n",
      "3       3.0  1970.0  ...  1984.0  1016.0   5.0  14.0  34.0   7.0  14.0   0.0   \n",
      "4       5.0  1991.0  ...  2009.0  1320.0   1.0  18.0   9.0   2.0  15.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "38330  12.0  1971.0  ...  1985.0  1032.0   1.0  14.0  34.0   7.0  12.0   3.0   \n",
      "38331   6.0  1988.0  ...  2002.0  1236.0   1.0  14.0  17.0   4.0  12.0   3.0   \n",
      "38332   7.0  1973.0  ...  1986.0  1043.0   1.0  13.0  33.0   7.0  12.0   4.0   \n",
      "38333  12.0  1994.0  ...  2008.0  1301.0   1.0  13.0  11.0   3.0  13.0   2.0   \n",
      "38334  10.0  1996.0  ...  2013.0  1362.0   1.0  16.0   6.0   2.0  15.0   2.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       1.0  11.0  \n",
      "1       NaN   NaN  \n",
      "2       1.0  14.0  \n",
      "3       0.0   6.0  \n",
      "4       0.0   6.0  \n",
      "...     ...   ...  \n",
      "38330   0.0   5.0  \n",
      "38331   0.0   7.0  \n",
      "38332   0.0   5.0  \n",
      "38333   0.0  11.0  \n",
      "38334   0.0  11.0  \n",
      "\n",
      "[38335 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "print(endes_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are combining the three dataframes rec1, rec2, and rec3 into a single dataframe called endes_2019. The merging is done based on the common column CASEID, which uniquely identifies each case in the survey.\n",
    "\n",
    "This process ensures that all relevant information from the three datasets is combined into one cohesive dataframe, making it easier to perform analyses and draw insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify variable labels into a single dictionary\n",
    "var_labels = new_var_labels1.copy()  # We create a copy of the dictionary new_var_labels1 to ensures that the original new_var_labels1 remains unchanged if any modifications are made to var_labels later.\n",
    "var_labels.update(new_var_labels2)  # Both lines update with variable labels from rec2 and rec3\n",
    "var_labels.update(new_var_labels3)  \n",
    "\n",
    "# Unify value labels into a single dictionary\n",
    "value_labels = new_value_labels1.copy()  # Copy the dictionary to avoid modifying the original\n",
    "value_labels.update(new_value_labels2) # Next both code lines update with value labels from rec2 and rec3\n",
    "value_labels.update(new_value_labels3)\n",
    "\n",
    "# Assign these dictionaries as attributes of the endes_2019 DataFrame\n",
    "endes_2019.attrs['var_labels'] = var_labels\n",
    "endes_2019.attrs['value_labels'] = value_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CASEID': 'Identificación Cuestionario Individual',\n",
       " 'V001': 'Conglomerado',\n",
       " 'V002': 'Número de vivienda',\n",
       " 'V003': 'Número de línea de entrevistada',\n",
       " 'V004': 'Unidad de área final',\n",
       " 'V007': 'Año de la entrevista',\n",
       " 'V008': 'Fecha de la entrevista, Codificación centenaria de meses (CMC)',\n",
       " 'V009': 'Mes de nacimiento de la entrevistada',\n",
       " 'V010': 'Año de nacimiento de la entrevistada',\n",
       " 'V011': 'Fecha de nacimiento, Codificación centenaria de meses (CMC)',\n",
       " 'V012': 'Edad actual - entrevistada',\n",
       " 'V024': 'Región',\n",
       " 'V000': 'Código y fase del país',\n",
       " 'V102': 'Tipo de lugar de residencia',\n",
       " 'V120': 'En su hogar tiene: radio',\n",
       " 'V121': 'En su hogar tiene: televisión',\n",
       " 'V122': 'En su hogar tiene: refrigerador',\n",
       " 'V123': 'En su hogar tiene: bicicleta',\n",
       " 'V124': 'En su hogar tiene: motocicleta/motocar',\n",
       " 'V125': 'En su hogar tiene: coche/camión',\n",
       " 'V127': 'Material predominante del piso de la vivienda',\n",
       " 'V133': 'Educación en años simples',\n",
       " 'year': 'Year of the survey',\n",
       " 'V201': 'Total de niños nacidos',\n",
       " 'V218': 'Número de niños vivos',\n",
       " 'V301': 'Conocimiento de cualquier método',\n",
       " 'V302': 'Alguna vez usó cualquier método',\n",
       " 'V323': 'Marca de la píldora usada',\n",
       " 'V323A': 'Marca del preservativo utilizado',\n",
       " 'V325A': 'Costo del método actual',\n",
       " 'V326': 'Fuente para obtener el actual método anticonceptivo',\n",
       " 'V327': 'Fuente para obtener el actual método anticonceptivo (Grupos)',\n",
       " 'V337': 'Meses de uso del método actual',\n",
       " 'V359': 'Última discontinuidad del método los últimos 5 años',\n",
       " 'V360': 'Motivo de la última discontinuidad',\n",
       " 'V361': 'Patrón de uso',\n",
       " 'V362': 'Intención de uso',\n",
       " 'V363': 'Método futuro preferido',\n",
       " 'V364': 'Uso e intención de anticonceptivos',\n",
       " 'V367': 'Quería quedar embarazada:',\n",
       " 'V372': 'Se muestra el paquete de pastillas',\n",
       " 'V372A': 'Se muestra el paquete de Condónes',\n",
       " 'V375A': 'La razón principal para no utilizar un método',\n",
       " 'V376': 'La razón principal por la que no piensa usar ningún método en el futuro',\n",
       " 'V376A': 'Alguna vez usaría algún método',\n",
       " 'V379': 'Fuente conocida por cualquier método',\n",
       " 'V380': 'Fuente conocida por cualquier método (Grupos)',\n",
       " 'V501': 'Estado civil actual',\n",
       " 'V502': 'Actualmente, antes o nunca sacada',\n",
       " 'V503': 'Usted ha estado casada o conviviendo solo una vez, más de una vez',\n",
       " 'V504': 'Su esposo/compañer vive en el hogar o vive en otro lugar',\n",
       " 'V505': 'Número de otras esposas',\n",
       " 'V506': 'Número de rango de la esposa',\n",
       " 'V507': 'Mes - Primer matrimonio',\n",
       " 'V508': 'Año - Primer matrimonio',\n",
       " 'V509': 'Fecha del primer matrimonio (CMC)',\n",
       " 'V510': 'Integridad de la información para la fecha de inicio del primer matrimonio o unión',\n",
       " 'V511': 'Edad al primer matrimonio',\n",
       " 'V512': 'Años desde el primer matrimonio',\n",
       " 'V513': 'Duración del matrimonio (agrupado)',\n",
       " 'V525': 'Edad en la primera relación sexual',\n",
       " 'V613': 'Número ideal de niños',\n",
       " 'V714': 'Actualmente se encuentra trabajando',\n",
       " 'V715': 'Educación en años individuales del esposo/compañero'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the content of the var_labels dictionary to verify the consolidation\n",
    "var_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V024': {1.0: 'Amazonas',\n",
       "  2.0: 'Ancash',\n",
       "  3.0: 'Apurimac',\n",
       "  4.0: 'Arequipa',\n",
       "  5.0: 'Ayacucho',\n",
       "  6.0: 'Cajamarca',\n",
       "  7.0: 'Callao',\n",
       "  8.0: 'Cusco',\n",
       "  9.0: 'Huancavelica',\n",
       "  10.0: ' Huanuco',\n",
       "  11.0: ' Ica',\n",
       "  12.0: ' Junin',\n",
       "  13.0: ' La Libertad',\n",
       "  14.0: ' Lambayeque',\n",
       "  15.0: ' Lima',\n",
       "  16.0: ' Loreto',\n",
       "  17.0: ' Madre de Dios',\n",
       "  18.0: ' Moquegua',\n",
       "  19.0: ' Pasco',\n",
       "  20.0: ' Piura',\n",
       "  21.0: ' Puno',\n",
       "  22.0: ' San Martin',\n",
       "  23.0: ' Tacna',\n",
       "  24.0: ' Tumbes',\n",
       "  25.0: ' Ucayali'},\n",
       " 'V102': {1.0: 'Urbano', 2.0: 'Rural'},\n",
       " 'V120': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V121': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V122': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V123': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V124': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V125': {0.0: 'No', 1.0: 'Si', 7.0: 'No es residente habitual'},\n",
       " 'V127': {11.0: 'Tierra/arena',\n",
       "  21.0: 'Madera (entablados)',\n",
       "  31.0: 'Parquet o madera pulida',\n",
       "  32.0: 'Làminas asfálticas, vinílicos o similares',\n",
       "  33.0: 'Losetas, terrazos o similares',\n",
       "  34.0: 'Cemento/ladrillo',\n",
       "  96.0: 'Pona',\n",
       "  97.0: 'Otro(pona)'},\n",
       " 'V133': {97.0: 'Inconsistente'},\n",
       " 'V301': {0.0: 'No conoce ningún método',\n",
       "  1.0: 'Sabe sólo folklórico',\n",
       "  2.0: 'Sólo sabe trad. mes',\n",
       "  3.0: 'Conoce el método moderno'},\n",
       " 'V302': {0.0: 'Nunca usado',\n",
       "  1.0: 'Otros metodos',\n",
       "  2.0: 'Metodos tradicionales',\n",
       "  3.0: 'Metodos modernos'},\n",
       " 'V323': {1.0: 'Microginon',\n",
       "  2.0: 'Nordet',\n",
       "  3.0: 'Lofemenal',\n",
       "  4.0: 'Gynera',\n",
       "  5.0: 'Ciclomex',\n",
       "  6.0: 'Triciclomex',\n",
       "  7.0: 'Femiane',\n",
       "  8.0: 'Mercilon',\n",
       "  9.0: 'Marvelon',\n",
       "  10.0: 'Lindiol-e',\n",
       "  11.0: 'Dione 35',\n",
       "  12.0: 'Cilest',\n",
       "  13.0: 'Triquilar',\n",
       "  14.0: 'Trinordiol',\n",
       "  15.0: 'Minigynon',\n",
       "  16.0: 'Exluton',\n",
       "  17.0: 'Ovrette',\n",
       "  18.0: 'Ovral',\n",
       "  19.0: 'Mirelle',\n",
       "  20.0: 'Yasmin Drospyperona',\n",
       "  21.0: 'Peldura',\n",
       "  22.0: 'Anuletyc',\n",
       "  23.0: 'Rigevidon',\n",
       "  24.0: 'Oral Con-F',\n",
       "  25.0: 'Canesa',\n",
       "  26.0: 'Belara',\n",
       "  27.0: 'Tri_Regol',\n",
       "  93.0: 'Sin marca (H. FF.AA., FF.PP)',\n",
       "  94.0: 'Sin marca (ESSALUD)',\n",
       "  95.0: 'Sin marca (MINSA)',\n",
       "  96.0: 'Otro',\n",
       "  98.0: 'No sabe'},\n",
       " 'V323A': {1.0: 'OK',\n",
       "  2.0: 'Piel',\n",
       "  3.0: 'Royal',\n",
       "  4.0: 'Jontex',\n",
       "  5.0: 'Contempo',\n",
       "  6.0: 'Gens',\n",
       "  7.0: 'LifeStyle',\n",
       "  8.0: 'Sultan',\n",
       "  9.0: 'Durex',\n",
       "  10.0: 'Preventor',\n",
       "  11.0: 'Hanson plans',\n",
       "  12.0: 'Sensitive',\n",
       "  13.0: 'Play boy',\n",
       "  14.0: 'Lubritec',\n",
       "  15.0: 'Pantera',\n",
       "  16.0: 'Tex',\n",
       "  17.0: 'Seohung',\n",
       "  18.0: 'Aladan corporation',\n",
       "  19.0: 'Vladi',\n",
       "  20.0: 'Stalion',\n",
       "  21.0: 'Latex',\n",
       "  22.0: 'Pussy Cat',\n",
       "  23.0: 'Sensación',\n",
       "  24.0: 'Karex',\n",
       "  25.0: 'Do it Loverly',\n",
       "  26.0: 'Nonoxinol - 9',\n",
       "  27.0: 'Famila 28',\n",
       "  93.0: 'Sin marca (H. FF.AA., FF.PP)',\n",
       "  94.0: 'Sin marca (ESSALUD)',\n",
       "  95.0: 'Sin marca (MINSA)',\n",
       "  96.0: 'Otro',\n",
       "  98.0: 'No sabe'},\n",
       " 'V325A': {9999995.0: 'Gratis', 9999998.0: 'No sabe'},\n",
       " 'V326': {11.0: 'Hospital MINSA',\n",
       "  12.0: 'Centro de salud MINSA',\n",
       "  13.0: 'Puesto de salud MINSA',\n",
       "  14.0: 'Promotor de salud MINSA',\n",
       "  15.0: 'Hospital ESSALUD',\n",
       "  16.0: 'Policlìnico/Centro/Posta ESSALUD',\n",
       "  17.0: 'Hospital/Otros de las FFAA & PNP',\n",
       "  18.0: 'Hospital/Otro de la Municipalidad',\n",
       "  19.0: 'Otro gobierno',\n",
       "  21.0: 'Clínica particular',\n",
       "  22.0: 'Farmacia/botica',\n",
       "  23.0: 'Consultorio médico particular',\n",
       "  24.0: 'Clínica/Posta de ONG',\n",
       "  25.0: 'Promotor de ONG',\n",
       "  31.0: 'Tienda/supermercado/hostal',\n",
       "  32.0: 'Hospital/Otro de la Iglesia',\n",
       "  33.0: 'Amigos/parientes',\n",
       "  95.0: 'Automedicación',\n",
       "  96.0: 'Otro',\n",
       "  98.0: 'No sabe'},\n",
       " 'V327': {1.0: 'Hospital, Centro de Salud, Puesto de Salud, Policlínico del Gobierno',\n",
       "  2.0: 'Promotor de Salud',\n",
       "  3.0: 'Clínica/Posta/ Promotor ONG',\n",
       "  4.0: 'Clínica/Consultorio médico particular',\n",
       "  5.0: 'Farmacia/botica',\n",
       "  6.0: 'Tienda, iglesia, amigo',\n",
       "  7.0: 'Automedicación/Otro',\n",
       "  8.0: 'No sabe',\n",
       "  9.0: 'Otro gobierno'},\n",
       " 'V337': {95.0: 'Todas cal {A core}',\n",
       "  96.0: '96+ {B core}',\n",
       "  997.0: 'Inconsistente',\n",
       "  998.0: 'No sabe'},\n",
       " 'V359': {1.0: 'Píldora',\n",
       "  2.0: 'DIU',\n",
       "  3.0: 'Inyecciones',\n",
       "  4.0: 'Diafragma',\n",
       "  5.0: 'Condón',\n",
       "  6.0: 'Esterilización femenina',\n",
       "  7.0: 'Esterilización masculina',\n",
       "  8.0: 'Abstinencia periódica',\n",
       "  9.0: 'Retiro',\n",
       "  10.0: 'Otro',\n",
       "  11.0: 'Norplant/Implantes',\n",
       "  12.0: 'Abstinencia',\n",
       "  13.0: 'Amenorrea por lactancia (MELA)',\n",
       "  14.0: 'Condón femenino',\n",
       "  15.0: 'Espuma, jalea, óvulos (vaginales)',\n",
       "  16.0: 'Anticoncepción oral de emergencia',\n",
       "  17.0: 'Método específico 1',\n",
       "  18.0: 'Método específico 2',\n",
       "  19.0: 'Método específico 3',\n",
       "  20.0: 'Método específico 4'},\n",
       " 'V360': {1.0: 'Quedó embarazada',\n",
       "  2.0: 'Quería quedar embarazada',\n",
       "  3.0: 'Marido desaprobó',\n",
       "  4.0: 'Efectos secundarios',\n",
       "  5.0: 'Motivos de salud',\n",
       "  6.0: 'Acceso, disponibilidad',\n",
       "  7.0: 'Deseaba método más efectivo',\n",
       "  8.0: 'Uso inconveniente',\n",
       "  9.0: 'Sexo poco frecuente/Marido ausente',\n",
       "  10.0: 'Costo',\n",
       "  11.0: 'Fatalista',\n",
       "  12.0: 'Dificil quedar embarazda/menopausia',\n",
       "  13.0: 'Disolución matrimonial',\n",
       "  14.0: 'Otro',\n",
       "  98.0: 'No sabe'},\n",
       " 'V361': {1.0: 'Actualmente usando',\n",
       "  2.0: 'Usado desde el último nacimiento',\n",
       "  3.0: 'Usado antes del último nacimiento',\n",
       "  4.0: 'Nunca usado'},\n",
       " 'V362': {1.0: 'En los próximos 12 meses',\n",
       "  2.0: 'Usar más tarde',\n",
       "  3.0: 'No está seguro sobre el momento',\n",
       "  4.0: 'No está seguro sobre el uso',\n",
       "  5.0: 'No tiene la intención',\n",
       "  6.0: 'Nunca tuve relaciones sexuales'},\n",
       " 'V363': {1.0: 'Píldora',\n",
       "  2.0: 'DIU',\n",
       "  3.0: 'Inyección',\n",
       "  4.0: 'Diafragma',\n",
       "  5.0: 'Condón',\n",
       "  6.0: 'Esterilización femenina',\n",
       "  7.0: 'Esterilización masculina',\n",
       "  8.0: 'Abstinencia periódica',\n",
       "  9.0: 'Retiro',\n",
       "  10.0: 'Otro',\n",
       "  11.0: 'Norplant/Implantes',\n",
       "  12.0: 'Abstinencia',\n",
       "  13.0: 'Amenorrea por lactancia (MELA)',\n",
       "  14.0: 'Condón femenino',\n",
       "  15.0: 'Espuma, jalea, óvulos (vaginales)',\n",
       "  16.0: 'Anticoncepción oral de emergencia',\n",
       "  98.0: 'No sabe'},\n",
       " 'V364': {1.0: 'Usando el método moderno',\n",
       "  2.0: 'Usando el método tradicional',\n",
       "  3.0: 'No-usuario tiene la intención de',\n",
       "  4.0: 'No tiene la intención de',\n",
       "  5.0: 'Nunca tuve relaciones sexuales'},\n",
       " 'V367': {1.0: 'Entonces', 2.0: 'Esperar más', 3.0: 'No quería más'},\n",
       " 'V372': {0.0: 'Paquete no visto', 1.0: 'Paquete visto'},\n",
       " 'V372A': {0.0: 'Paquete no visto', 1.0: 'Paquete visto'},\n",
       " 'V375A': {11.0: 'No casado',\n",
       "  21.0: 'No tiene relaciones sexuales',\n",
       "  22.0: 'Sexo poco frecuente',\n",
       "  23.0: 'Menopausia',\n",
       "  24.0: 'Infertilidad/Subfecundidad',\n",
       "  25.0: 'Postparto/Lactancia',\n",
       "  26.0: 'Quiere más hijas(os)',\n",
       "  27.0: 'Embarazada',\n",
       "  28.0: 'Histerectomìa',\n",
       "  31.0: 'Entrevistada se opone',\n",
       "  32.0: 'Esposo/compañero se opuso',\n",
       "  33.0: 'Otros se oponen',\n",
       "  34.0: 'Prohibición religiosa',\n",
       "  41.0: 'No conoce ningún método',\n",
       "  42.0: 'No conoce ninguna fuente',\n",
       "  51.0: 'Problemas de salud',\n",
       "  52.0: 'Miedo a efectos secundarios',\n",
       "  53.0: 'Falta de acceso',\n",
       "  54.0: 'Muy costoso',\n",
       "  55.0: 'Uso inconveniente',\n",
       "  56.0: 'Interfiere con procesos normales del cuerpo',\n",
       "  96.0: 'Otro',\n",
       "  98.0: 'No sabe'},\n",
       " 'V376': {11.0: 'No casado',\n",
       "  22.0: 'Sexo poco frecuente',\n",
       "  23.0: 'Menopáusica/histerectomizada',\n",
       "  24.0: 'Infertilidad mujer',\n",
       "  25.0: 'Infertilidad hombre',\n",
       "  26.0: 'Quiere más hijas(os)',\n",
       "  31.0: 'Entrevistada se opone',\n",
       "  32.0: 'Esposo/compañero se opuso',\n",
       "  33.0: 'Otros se oponen',\n",
       "  34.0: 'Prohibición religiosa',\n",
       "  41.0: 'No conoce ningún método',\n",
       "  42.0: 'No conoce ninguna fuente',\n",
       "  51.0: 'Problemas de salud',\n",
       "  52.0: 'Miedo a efectos secundarios',\n",
       "  53.0: 'Falta de acceso',\n",
       "  54.0: 'Muy costoso',\n",
       "  55.0: 'Uso inconveniente',\n",
       "  56.0: 'Interfiere con procesos normales del cuerpo',\n",
       "  96.0: 'Otro',\n",
       "  98.0: 'No sabe'},\n",
       " 'V376A': {0.0: 'No', 1.0: 'Si', 8.0: 'No sabe'},\n",
       " 'V379': {11.0: 'Hospital MINSA',\n",
       "  12.0: 'Centro de salud MINSA',\n",
       "  13.0: 'Puesto de salud MINSA',\n",
       "  14.0: 'Trabajadora sanitaria MINSA',\n",
       "  15.0: 'Hospital ESSALUD',\n",
       "  16.0: 'Centro/Correo ESSALUD',\n",
       "  17.0: 'Campaña/feria',\n",
       "  18.0: 'Otro gobierno',\n",
       "  21.0: 'Clínica privada',\n",
       "  22.0: 'Farmacia',\n",
       "  23.0: 'Médico privado',\n",
       "  24.0: 'Clínica/FP post ONG',\n",
       "  25.0: 'ONG de trabajadores de la salud',\n",
       "  31.0: 'Tienda/supermercado',\n",
       "  32.0: 'Iglesia',\n",
       "  33.0: 'Amigos/parientes',\n",
       "  95.0: 'Automedicación',\n",
       "  96.0: 'Otro',\n",
       "  98.0: 'No sabe'},\n",
       " 'V380': {1.0: 'Govt Clinical/Pharm',\n",
       "  2.0: 'Casa de Gobierno/Comm liv',\n",
       "  3.0: 'ONG',\n",
       "  4.0: 'Private Clin/Deliv',\n",
       "  5.0: 'Farmacia Privada',\n",
       "  6.0: 'Tienda, iglesia, amigo',\n",
       "  7.0: 'Otro',\n",
       "  8.0: 'No sabe'},\n",
       " 'V501': {0.0: 'Nunca casada',\n",
       "  1.0: 'Casado',\n",
       "  2.0: 'Viviendo juntos',\n",
       "  3.0: 'Viuda',\n",
       "  4.0: 'Divorciada',\n",
       "  5.0: 'No viven juntos'},\n",
       " 'V502': {0.0: 'Nunca casada',\n",
       "  1.0: 'Actualmente casada',\n",
       "  2.0: 'Anteriormente casada'},\n",
       " 'V503': {1.0: 'Una vez', 2.0: 'Mas de una vez'},\n",
       " 'V504': {1.0: 'Vive con ella', 2.0: 'Vive en otro sitio'},\n",
       " 'V505': {0.0: 'No otras esposas', 98.0: 'No sabe'},\n",
       " 'V506': {98.0: 'No sabe'},\n",
       " 'V510': {1.0: 'Tiene mes y año',\n",
       "  2.0: 'Tiene mes y edad - imputa año',\n",
       "  3.0: 'Tiene año y edad - imputa mes',\n",
       "  5.0: 'Tiene año - imputa mes',\n",
       "  6.0: 'Tiene edad - imputa mes año',\n",
       "  7.0: 'Tiene mes - imputa año',\n",
       "  8.0: 'Ninguno - imputa todo'},\n",
       " 'V513': {0.0: 'Nunca casada',\n",
       "  1.0: 'De 0 a 4 años',\n",
       "  2.0: 'De 5 a 9 años',\n",
       "  3.0: 'De 10 a 14 años',\n",
       "  4.0: 'De 15 a 19 años',\n",
       "  5.0: 'De 20 a 24 años',\n",
       "  6.0: 'De 25 a 29 años',\n",
       "  7.0: 'De 30 a más años'},\n",
       " 'V525': {0.0: 'Nunca tuvo relaciones sexuales', 96.0: 'En la primera union'},\n",
       " 'V613': {96.0: 'Respuesta no numérica'},\n",
       " 'V714': {0.0: 'No', 1.0: 'Sí'},\n",
       " 'V715': {98.0: 'No sabe'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, replicate your code of the prevoius sections but for years **2019, 2018, 2017, 2016, 2015**. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their **variables and values labels** from this path `\"../../_data/endes/\"`. For this excersie you must use a for loop. This loop must iterate over **2019, 2018, 2017, 2016, 2015 folders** and import these files. All the files have the same name. You must store these files and their labels in a nested dictionary named as `all_data`. The keys of the dictionary should be named as `year_2019`, for example, and the keys of the nested dictionary should be `data`, `var_labels`, and `value_labels`. **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-50f8d6d7df09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Reading the .sav file and its metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyreadstat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sav\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Extracting the base name (without the .sav extension) for use as the key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpyreadstat\\\\pyreadstat.pyx\u001b[0m in \u001b[0;36mpyreadstat.pyreadstat.read_sav\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpyreadstat\\\\_readstat_parser.pyx\u001b[0m in \u001b[0;36mpyreadstat._readstat_parser.run_conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpyreadstat\\\\_readstat_parser.pyx\u001b[0m in \u001b[0;36mpyreadstat._readstat_parser.dict_to_pandas_dataframe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1357\u001b[0m         \"\"\"\n\u001b[0;32m   1358\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[0morient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"index\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List of years to process.\n",
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "\n",
    "# Initializing the dictionary that will hold the results\n",
    "all_data = {}\n",
    "\n",
    "# List of filenames to be imported\n",
    "files = ['REC0111.sav', 'RE223132.sav', 'RE516171.sav']\n",
    "\n",
    "# Looping through each year to load the files and their labels\n",
    "for year in years:\n",
    "    # Creating the key for the current year in the dictionary\n",
    "    year_key = f'year_{year}'\n",
    "    \n",
    "    # Setting up the nested dictionary structure for the current year\n",
    "    all_data[year_key] = {\n",
    "        'data': {},\n",
    "        'var_labels': {},\n",
    "        'value_labels': {}\n",
    "    }\n",
    "    \n",
    "    # Looping through each file to load data and labels\n",
    "    for file in files:\n",
    "        # Building the file path\n",
    "        file_path = os.path.join(f'../../_data/endes/{year}', file)\n",
    "        \n",
    "        # Reading the .sav file and its metadata\n",
    "        data, meta = pyreadstat.read_sav(file_path)\n",
    "        \n",
    "        # Extracting the base name (without the .sav extension) for use as the key\n",
    "        file_key = os.path.splitext(file)[0].lower()\n",
    "        \n",
    "        # Storing the data and labels in their respective dictionaries\n",
    "        all_data[year_key]['data'][file_key] = data\n",
    "        all_data[year_key]['var_labels'][file_key] = meta.column_names_to_labels\n",
    "        all_data[year_key]['value_labels'][file_key] = meta.variable_value_labels\n",
    "\n",
    "# Displaying the resulting dictionary\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `all_data` to append all the data sets. Store all data sets in a list using `for loop`. Then, use `pd.concat` to append all the data sets. Also, you must reset the index to have a good-looking data. This new object should be named as `endes_data_2015_2019`. **Hint: Use [this code](https://stackoverflow.com/questions/32444138/concatenate-a-list-of-pandas-dataframes-together)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path where the files are located\n",
    "file_path = \"../../_data/endes/2019/\"\n",
    "\n",
    "# Import .sav files\n",
    "rec_1, meta_1 = pyreadstat.read_sav(file_path + \"REC0111.sav\")\n",
    "rec_2, meta_2 = pyreadstat.read_sav(file_path + \"RE223132.sav\")\n",
    "rec_3, meta_3 = pyreadstat.read_sav(file_path + \"RE516171.sav\")\n",
    "\n",
    "# Extracting variable labels and values from Metadata objects\n",
    "var_labels1 = meta_1.variable_labels if hasattr(meta_1, 'variable_labels') else {}\n",
    "value_labels1 = meta_1.value_labels if hasattr(meta_1, 'value_labels') else {}\n",
    "\n",
    "var_labels2 = meta_2.variable_labels if hasattr(meta_2, 'variable_labels') else {}\n",
    "value_labels2 = meta_2.value_labels if hasattr(meta_2, 'value_labels') else {}\n",
    "\n",
    "var_labels3 = meta_3.variable_labels if hasattr(meta_3, 'variable_labels') else {}\n",
    "value_labels3 = meta_3.value_labels if hasattr(meta_3, 'value_labels') else {}\n",
    "\n",
    "print(\"Variable labels rec_1:\", var_labels1)\n",
    "print(\"Value labels rec_1:\", value_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired columns for each dataset\n",
    "columns_rec1 = ['CASEID', 'V000', 'V001', 'V002', 'V003', 'V004', 'V007', 'V008', 'V009', 'V010', 'V011', 'V012', 'V024', 'V102', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V127', 'V133']\n",
    "columns_rec2 = ['CASEID', 'V201', 'V218', 'V301', 'V302', 'V323', 'V323A', 'V325A', 'V326', 'V327', 'V337', 'V359', 'V360', 'V361', 'V362', 'V363', 'V364', 'V367', 'V372', 'V372A', 'V375A', 'V376', 'V376A', 'V379', 'V380']\n",
    "columns_rec3 = ['CASEID', 'V501', 'V502', 'V503', 'V504', 'V505', 'V506', 'V507', 'V508', 'V509', 'V510', 'V511', 'V512', 'V513', 'V525', 'V613', 'V714', 'V715']\n",
    "\n",
    "# Select columns and check if all columns are present\n",
    "rec1_1 = rec_1.loc[:, columns_rec1]\n",
    "rec2_1 = rec_2.loc[:, columns_rec2]\n",
    "rec3_1 = rec_3.loc[:, columns_rec3]\n",
    "\n",
    "missing_columns_rec1 = [col for col in columns_rec1 if col not in rec_1.columns]\n",
    "missing_columns_rec2 = [col for col in columns_rec2 if col not in rec_2.columns]\n",
    "missing_columns_rec3 = [col for col in columns_rec3 if col not in rec_3.columns]\n",
    "\n",
    "print(\"Missing columns in rec1:\", missing_columns_rec1)\n",
    "print(\"Missing columns in rec2:\", missing_columns_rec2)\n",
    "print(\"Missing columns in rec3:\", missing_columns_rec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'year' column without SettingWithCopyWarning warning\n",
    "rec1_1.loc[:, 'year'] = 2019\n",
    "rec2_1.loc[:, 'year'] = 2019\n",
    "rec3_1.loc[:, 'year'] = 2019\n",
    "\n",
    "# Merge dataframes into a single frame using CASEID\n",
    "endes_2019 = pd.merge(pd.merge(rec1_1, rec2_1, on='CASEID'), rec3_1, on='CASEID')\n",
    "\n",
    "# Apply labels to new attributes\n",
    "new_var_labels1 = {k: var_labels1.get(k, \"Unknown label\") for k in columns_rec1}\n",
    "new_value_labels1 = {k: value_labels1.get(k, \"Unknown label\") for k in columns_rec1}\n",
    "\n",
    "new_var_labels2 = {k: var_labels2.get(k, \"Unknown label\") for k in columns_rec2}\n",
    "new_value_labels2 = {k: value_labels2.get(k, \"Unknown label\") for k in columns_rec2}\n",
    "\n",
    "new_var_labels3 = {k: var_labels3.get(k, \"Unknown label\") for k in columns_rec3}\n",
    "new_value_labels3 = {k: value_labels3.get(k, \"Unknown label\") for k in columns_rec3}\n",
    "\n",
    "endes_2019.attrs['var_labels'] = {**new_var_labels1, **new_var_labels2, **new_var_labels3}\n",
    "endes_2019.attrs['value_labels'] = {**new_value_labels1, **new_value_labels2, **new_value_labels3}\n",
    "\n",
    "print(endes_2019.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Store all the `var_labels` and `value_labels` in a dictionary named as `all_var_labels` and `all_value_labels`. The first keys should be the year for both dictionaries.Then, use them to generate new attributes for `endes_data_2015_2019`. These attributes should be named as `var_labels` and `value_labels`.  **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.3.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base route and years\n",
    "base_path = \"../../_data/endes/\"\n",
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "all_data = {}\n",
    "\n",
    "for year in years:\n",
    "    year_path = os.path.join(base_path, str(year))\n",
    "    \n",
    "    # Import .sav files for each year\n",
    "    rec_1, meta_1 = pyreadstat.read_sav(os.path.join(year_path, \"REC0111.sav\"))\n",
    "    rec_2, meta_2 = pyreadstat.read_sav(os.path.join(year_path, \"RE223132.sav\"))\n",
    "    rec_3, meta_3 = pyreadstat.read_sav(os.path.join(year_path, \"RE516171.sav\"))\n",
    "\n",
    "    # Verify and select columns\n",
    "    available_columns_rec1 = [col for col in columns_rec1 if col in rec_1.columns]\n",
    "    available_columns_rec2 = [col for col in columns_rec2 if col in rec_2.columns]\n",
    "    available_columns_rec3 = [col for col in columns_rec3 if col in rec_3.columns]\n",
    "\n",
    "    rec_1 = rec_1.loc[:, available_columns_rec1]\n",
    "    rec_2 = rec_2.loc[:, available_columns_rec2]\n",
    "    rec_3 = rec_3.loc[:, available_columns_rec3]\n",
    "\n",
    "    # Add 'year' column\n",
    "    rec_1['year'] = year\n",
    "    rec_2['year'] = year\n",
    "    rec_3['year'] = year\n",
    "\n",
    "    # Merge dataframes into a single frame using CASEID\n",
    "    endes_year = pd.merge(pd.merge(rec_1, rec_2, on='CASEID', how='inner'), rec_3, on='CASEID', how='inner')\n",
    "\n",
    "    # Data Storage\n",
    "    all_data[f'year_{year}'] = {\n",
    "        'data': endes_year,\n",
    "        'var_labels': {\n",
    "            **new_var_labels1,\n",
    "            **new_var_labels2,\n",
    "            **new_var_labels3\n",
    "        },\n",
    "        'value_labels': {\n",
    "            **new_value_labels1,\n",
    "            **new_value_labels2,\n",
    "            **new_value_labels3\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Data for all years imported and prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames\n",
    "data_frames = [info['data'] for info in all_data.values()]\n",
    "endes_data_2015_2019 = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Apply labels to new attributes\n",
    "all_var_labels = {year: info['var_labels'] for year, info in all_data.items()}\n",
    "all_value_labels = {year: info['value_labels'] for year, info in all_data.items()}\n",
    "\n",
    "endes_data_2015_2019.attrs['var_labels'] = all_var_labels\n",
    "endes_data_2015_2019.attrs['value_labels'] = all_value_labels\n",
    "\n",
    "print(\"Final concatenated DataFrame and applied tags.\")\n",
    "print(endes_data_2015_2019.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate averages\n",
    "mean_key_vars = endes_data_2015_2019.groupby(['year', 'V024']).agg(\n",
    "    mean_total_children=('V201', 'mean'),\n",
    "    mean_ideal_children=('V613', 'mean'),\n",
    "    mean_hb_yr_educ=('V715', 'mean'),\n",
    "    mean_first_marriage=('V511', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Merge with the original DataFrame\n",
    "final_result = pd.merge(endes_data_2015_2019, mean_key_vars, on=['year', 'V024'])\n",
    "print(\"Final data with averages calculated and merged.\")\n",
    "print(final_result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Use `endes_data_2015_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by year and department (V201) using endes_data_2015_2019\n",
    "#.agg to add de mean of the selected columns\n",
    "#save it in a new object: mean_key_vars\n",
    "\n",
    "mean_key_vars = endes_data_2015_2019.groupby(['year', 'V024']).agg({\n",
    "    'V201': 'mean',\n",
    "    'V613': 'mean',\n",
    "    'V715': 'mean',\n",
    "    'V511': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "#rename the columns as mean_ ...\n",
    "mean_key_vars.rename(columns={\n",
    "    'V201': 'mean_total_children',\n",
    "    'V613': 'mean_ideal_children',\n",
    "    'V715': 'mean_hb_yr_educ',\n",
    "    'V511': 'mean_first_marriage'\n",
    "}, inplace=True)\n",
    "\n",
    "#view mean_key_vars\n",
    "mean_key_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `mean_key_vars` with `endes_data_2015_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge both data frames and name the object final_result. \n",
    "#Merge considering year and department (V024)\n",
    "#Use how=left to mantain all the rows of endes_data_2015_2019 and add the columns of mean_key_vars.\n",
    "\n",
    "final_result = pd.merge(endes_data_2015_2019, mean_key_vars, on=['year', 'V024'], how='left')\n",
    "\n",
    "#view the final dataframe\n",
    "final_result"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
